2022-03-07 21:46:54,990 config: Namespace(K=256, M=4, T=0.45, alpha=10, batch_size=128, checkpoint_root='./checkpoints/Flickr32bits', dataset='Flickr25K', device='cuda:0', download_cifar10=False, epoch_num=50, eval_interval=1, feat_dim=64, final_lr=1e-05, hp_beta=0.1, hp_gamma=0.5, hp_lambda=1.0, is_asym_dist=True, lr=0.01, lr_scaling=0.001, mode='debias', momentum=0.9, monitor_counter=10, notes='Flickr32bits', num_workers=20, optimizer='SGD', pos_prior=0.15, protocal='I', queue_begin_epoch=5, seed=2021, start_lr=1e-05, topK=5000, trainable_layer_num=0, use_scheduler=True, use_writer=True, vgg_model_path='vgg16.pth', warmup_epoch_num=1).
2022-03-07 21:46:54,990 prepare Flickr25K datatset.
2022-03-07 21:46:55,673 setup model.
2022-03-07 22:03:49,947 config: Namespace(K=256, M=4, T=0.45, alpha=10, batch_size=128, checkpoint_root='./checkpoints/Flickr32bits', dataset='Flickr25K', device='cuda:0', download_cifar10=False, epoch_num=50, eval_interval=1, feat_dim=64, final_lr=1e-05, hp_beta=0.1, hp_gamma=0.5, hp_lambda=1.0, is_asym_dist=True, lr=0.01, lr_scaling=0.001, mode='debias', momentum=0.9, monitor_counter=10, notes='Flickr32bits', num_workers=20, optimizer='SGD', pos_prior=0.15, protocal='I', queue_begin_epoch=5, seed=2021, start_lr=1e-05, topK=5000, trainable_layer_num=0, use_scheduler=True, use_writer=True, vgg_model_path='vgg16.pth', warmup_epoch_num=1).
2022-03-07 22:03:49,959 prepare Flickr25K datatset.
2022-03-07 22:03:51,090 setup model.
2022-03-07 22:04:05,005 define loss function.
2022-03-07 22:04:05,006 setup SGD optimizer.
2022-03-07 22:04:05,009 prepare monitor and evaluator.
2022-03-07 22:04:05,010 begin to train model.
2022-03-07 22:04:05,012 register queue.
2022-03-07 22:05:57,464 epoch 0: avg loss=7.228348, avg quantization error=0.017816.
2022-03-07 22:05:57,464 begin to evaluate model.
2022-03-07 22:11:37,632 compute mAP.
2022-03-07 22:12:17,808 val mAP=0.799076.
2022-03-07 22:12:17,810 save the best model, db_codes and db_targets.
2022-03-07 22:12:21,220 finish saving.
2022-03-07 22:13:13,293 epoch 1: avg loss=5.400227, avg quantization error=0.013776.
2022-03-07 22:13:13,294 begin to evaluate model.
2022-03-07 22:13:58,161 compute mAP.
2022-03-07 22:14:08,235 val mAP=0.808821.
2022-03-07 22:14:08,236 save the best model, db_codes and db_targets.
2022-03-07 22:14:12,027 finish saving.
2022-03-07 22:15:01,102 epoch 2: avg loss=5.020596, avg quantization error=0.013306.
2022-03-07 22:15:01,103 begin to evaluate model.
2022-03-07 22:15:45,174 compute mAP.
2022-03-07 22:15:54,097 val mAP=0.806655.
2022-03-07 22:15:54,101 the monitor loses its patience to 9!.
2022-03-07 22:16:41,651 epoch 3: avg loss=4.941952, avg quantization error=0.013025.
2022-03-07 22:16:41,652 begin to evaluate model.
2022-03-07 22:17:24,168 compute mAP.
2022-03-07 22:17:33,405 val mAP=0.810789.
2022-03-07 22:17:33,409 save the best model, db_codes and db_targets.
2022-03-07 22:17:37,395 finish saving.
2022-03-07 22:18:28,874 epoch 4: avg loss=4.877846, avg quantization error=0.012804.
2022-03-07 22:18:28,875 begin to evaluate model.
2022-03-07 22:19:13,557 compute mAP.
2022-03-07 22:19:23,707 val mAP=0.809133.
2022-03-07 22:19:23,708 the monitor loses its patience to 9!.
2022-03-07 22:20:19,649 epoch 5: avg loss=7.442838, avg quantization error=0.012289.
2022-03-07 22:20:19,650 begin to evaluate model.
2022-03-07 22:21:07,038 compute mAP.
2022-03-07 22:21:17,303 val mAP=0.807647.
2022-03-07 22:21:17,304 the monitor loses its patience to 8!.
2022-03-07 22:22:02,218 epoch 6: avg loss=7.381712, avg quantization error=0.011535.
2022-03-07 22:22:02,220 begin to evaluate model.
2022-03-07 22:22:47,531 compute mAP.
2022-03-07 22:22:56,993 val mAP=0.796826.
2022-03-07 22:22:56,994 the monitor loses its patience to 7!.
2022-03-07 22:23:45,881 epoch 7: avg loss=7.379157, avg quantization error=0.011305.
2022-03-07 22:23:45,882 begin to evaluate model.
2022-03-07 22:24:29,916 compute mAP.
2022-03-07 22:24:39,785 val mAP=0.797643.
2022-03-07 22:24:39,789 the monitor loses its patience to 6!.
2022-03-07 22:25:19,168 epoch 8: avg loss=7.321486, avg quantization error=0.010795.
2022-03-07 22:25:19,169 begin to evaluate model.
2022-03-07 22:26:04,443 compute mAP.
2022-03-07 22:26:14,257 val mAP=0.794415.
2022-03-07 22:26:14,258 the monitor loses its patience to 5!.
2022-03-07 22:27:04,765 epoch 9: avg loss=7.347661, avg quantization error=0.010652.
2022-03-07 22:27:04,766 begin to evaluate model.
2022-03-07 22:27:49,888 compute mAP.
2022-03-07 22:27:59,666 val mAP=0.802387.
2022-03-07 22:27:59,667 the monitor loses its patience to 4!.
2022-03-07 22:28:51,708 epoch 10: avg loss=7.330157, avg quantization error=0.010094.
2022-03-07 22:28:51,711 begin to evaluate model.
2022-03-07 22:29:36,219 compute mAP.
2022-03-07 22:29:45,901 val mAP=0.789632.
2022-03-07 22:29:45,902 the monitor loses its patience to 3!.
2022-03-07 22:30:37,062 epoch 11: avg loss=7.348012, avg quantization error=0.009524.
2022-03-07 22:30:37,065 begin to evaluate model.
2022-03-07 22:31:20,905 compute mAP.
2022-03-07 22:31:30,710 val mAP=0.799365.
2022-03-07 22:31:30,712 the monitor loses its patience to 2!.
2022-03-07 22:32:19,062 epoch 12: avg loss=7.328611, avg quantization error=0.009184.
2022-03-07 22:32:19,063 begin to evaluate model.
2022-03-07 22:33:01,419 compute mAP.
2022-03-07 22:33:11,534 val mAP=0.798301.
2022-03-07 22:33:11,535 the monitor loses its patience to 1!.
2022-03-07 22:33:55,670 epoch 13: avg loss=7.356002, avg quantization error=0.009035.
2022-03-07 22:33:55,671 begin to evaluate model.
2022-03-07 22:34:40,650 compute mAP.
2022-03-07 22:34:51,490 val mAP=0.807983.
2022-03-07 22:34:51,491 the monitor loses its patience to 0!.
2022-03-07 22:34:51,491 early stop.
2022-03-07 22:34:51,492 free the queue memory.
2022-03-07 22:34:51,492 finish trainning at epoch 13.
2022-03-07 22:34:51,495 finish training, now load the best model and codes.
2022-03-07 22:34:53,064 begin to test model.
2022-03-07 22:34:53,065 compute mAP.
2022-03-07 22:35:03,476 test mAP=0.810789.
2022-03-07 22:35:03,477 compute PR curve and P@top5000 curve.
2022-03-07 22:35:23,151 finish testing.
2022-03-07 22:35:23,152 finish all procedures.
