2022-03-07 21:43:45,305 config: Namespace(K=256, M=2, T=0.35, alpha=10, batch_size=128, checkpoint_root='./checkpoints/CifarII16bitsSymm', dataset='CIFAR10', device='cuda:0', download_cifar10=False, epoch_num=50, eval_interval=1, feat_dim=16, final_lr=1e-05, hp_beta=0.01, hp_gamma=0.5, hp_lambda=0.01, is_asym_dist=False, lr=0.01, lr_scaling=0.001, mode='debias', momentum=0.9, monitor_counter=10, notes='CifarII16bitsSymm', num_workers=20, optimizer='SGD', pos_prior=0.1, protocal='II', queue_begin_epoch=15, seed=2021, start_lr=1e-05, topK=1000, trainable_layer_num=2, use_scheduler=True, use_writer=True, vgg_model_path='vgg16.pth', warmup_epoch_num=1).
2022-03-07 21:43:45,306 prepare CIFAR10 datatset.
2022-03-07 21:43:46,838 setup model.
2022-03-07 21:43:54,169 define loss function.
2022-03-07 21:43:54,170 setup SGD optimizer.
2022-03-07 21:43:54,171 prepare monitor and evaluator.
2022-03-07 21:43:54,172 begin to train model.
2022-03-07 21:43:54,173 register queue.
2022-03-07 21:44:16,629 epoch 0: avg loss=4.055814, avg quantization error=0.016865.
2022-03-07 21:44:16,629 begin to evaluate model.
2022-03-07 21:45:32,236 compute mAP.
2022-03-07 21:45:49,414 val mAP=0.478888.
2022-03-07 21:45:49,427 save the best model, db_codes and db_targets.
2022-03-07 21:45:52,574 finish saving.
2022-03-07 21:46:13,570 epoch 1: avg loss=3.050077, avg quantization error=0.016092.
2022-03-07 21:46:13,571 begin to evaluate model.
2022-03-07 21:47:29,031 compute mAP.
2022-03-07 21:47:46,196 val mAP=0.512169.
2022-03-07 21:47:46,197 save the best model, db_codes and db_targets.
2022-03-07 21:47:56,068 finish saving.
2022-03-07 21:48:16,917 epoch 2: avg loss=2.847339, avg quantization error=0.015550.
2022-03-07 21:48:16,926 begin to evaluate model.
2022-03-07 21:49:31,878 compute mAP.
2022-03-07 21:49:49,261 val mAP=0.523444.
2022-03-07 21:49:49,262 save the best model, db_codes and db_targets.
2022-03-07 21:49:51,912 finish saving.
2022-03-07 21:50:12,649 epoch 3: avg loss=2.691748, avg quantization error=0.015395.
2022-03-07 21:50:12,649 begin to evaluate model.
2022-03-07 21:51:30,073 compute mAP.
2022-03-07 21:51:47,456 val mAP=0.538370.
2022-03-07 21:51:47,457 save the best model, db_codes and db_targets.
2022-03-07 21:51:50,035 finish saving.
2022-03-07 21:52:11,257 epoch 4: avg loss=2.549976, avg quantization error=0.015271.
2022-03-07 21:52:11,258 begin to evaluate model.
2022-03-07 21:53:28,753 compute mAP.
2022-03-07 21:53:46,001 val mAP=0.544220.
2022-03-07 21:53:46,001 save the best model, db_codes and db_targets.
2022-03-07 21:53:48,865 finish saving.
2022-03-07 21:54:09,790 epoch 5: avg loss=2.439994, avg quantization error=0.015352.
2022-03-07 21:54:09,791 begin to evaluate model.
2022-03-07 21:55:26,692 compute mAP.
2022-03-07 21:55:44,291 val mAP=0.543397.
2022-03-07 21:55:44,292 the monitor loses its patience to 9!.
2022-03-07 21:56:05,499 epoch 6: avg loss=2.388028, avg quantization error=0.015227.
2022-03-07 21:56:05,500 begin to evaluate model.
2022-03-07 21:57:23,091 compute mAP.
2022-03-07 21:57:39,943 val mAP=0.549630.
2022-03-07 21:57:39,944 save the best model, db_codes and db_targets.
2022-03-07 21:57:42,875 finish saving.
2022-03-07 21:58:03,802 epoch 7: avg loss=2.337505, avg quantization error=0.015152.
2022-03-07 21:58:03,803 begin to evaluate model.
2022-03-07 21:59:20,220 compute mAP.
2022-03-07 21:59:37,435 val mAP=0.567062.
2022-03-07 21:59:37,436 save the best model, db_codes and db_targets.
2022-03-07 21:59:40,013 finish saving.
2022-03-07 22:00:01,297 epoch 8: avg loss=2.253923, avg quantization error=0.015053.
2022-03-07 22:00:01,298 begin to evaluate model.
2022-03-07 22:01:17,996 compute mAP.
2022-03-07 22:01:35,425 val mAP=0.571713.
2022-03-07 22:01:35,426 save the best model, db_codes and db_targets.
2022-03-07 22:01:37,951 finish saving.
2022-03-07 22:01:58,778 epoch 9: avg loss=2.259105, avg quantization error=0.014848.
2022-03-07 22:01:58,778 begin to evaluate model.
2022-03-07 22:03:15,312 compute mAP.
2022-03-07 22:03:32,332 val mAP=0.573517.
2022-03-07 22:03:32,333 save the best model, db_codes and db_targets.
2022-03-07 22:03:34,902 finish saving.
2022-03-07 22:03:55,747 epoch 10: avg loss=2.202210, avg quantization error=0.015043.
2022-03-07 22:03:55,747 begin to evaluate model.
2022-03-07 22:05:11,956 compute mAP.
2022-03-07 22:05:29,451 val mAP=0.572339.
2022-03-07 22:05:29,452 the monitor loses its patience to 9!.
2022-03-07 22:05:50,113 epoch 11: avg loss=2.139205, avg quantization error=0.014865.
2022-03-07 22:05:50,114 begin to evaluate model.
2022-03-07 22:07:07,258 compute mAP.
2022-03-07 22:07:24,353 val mAP=0.580340.
2022-03-07 22:07:24,353 save the best model, db_codes and db_targets.
2022-03-07 22:07:27,019 finish saving.
2022-03-07 22:07:48,374 epoch 12: avg loss=2.106568, avg quantization error=0.014904.
2022-03-07 22:07:48,375 begin to evaluate model.
2022-03-07 22:09:05,221 compute mAP.
2022-03-07 22:09:22,165 val mAP=0.576949.
2022-03-07 22:09:22,166 the monitor loses its patience to 9!.
2022-03-07 22:09:43,121 epoch 13: avg loss=2.061913, avg quantization error=0.014829.
2022-03-07 22:09:43,122 begin to evaluate model.
2022-03-07 22:10:59,169 compute mAP.
2022-03-07 22:11:15,980 val mAP=0.580039.
2022-03-07 22:11:15,980 the monitor loses its patience to 8!.
2022-03-07 22:11:36,766 epoch 14: avg loss=2.037777, avg quantization error=0.014829.
2022-03-07 22:11:36,767 begin to evaluate model.
2022-03-07 22:12:53,630 compute mAP.
2022-03-07 22:13:10,932 val mAP=0.581715.
2022-03-07 22:13:10,932 save the best model, db_codes and db_targets.
2022-03-07 22:13:24,664 finish saving.
2022-03-07 22:13:45,292 epoch 15: avg loss=4.363676, avg quantization error=0.015123.
2022-03-07 22:13:45,293 begin to evaluate model.
2022-03-07 22:15:01,672 compute mAP.
2022-03-07 22:15:19,246 val mAP=0.580404.
2022-03-07 22:15:19,247 the monitor loses its patience to 9!.
2022-03-07 22:15:39,675 epoch 16: avg loss=4.332609, avg quantization error=0.015068.
2022-03-07 22:15:39,675 begin to evaluate model.
2022-03-07 22:16:55,951 compute mAP.
2022-03-07 22:17:13,630 val mAP=0.581720.
2022-03-07 22:17:13,631 save the best model, db_codes and db_targets.
2022-03-07 22:17:16,286 finish saving.
2022-03-07 22:17:37,277 epoch 17: avg loss=4.314008, avg quantization error=0.015070.
2022-03-07 22:17:37,278 begin to evaluate model.
2022-03-07 22:18:53,611 compute mAP.
2022-03-07 22:19:11,055 val mAP=0.589308.
2022-03-07 22:19:11,055 save the best model, db_codes and db_targets.
2022-03-07 22:19:13,650 finish saving.
2022-03-07 22:19:34,671 epoch 18: avg loss=4.292591, avg quantization error=0.015176.
2022-03-07 22:19:34,672 begin to evaluate model.
2022-03-07 22:20:51,594 compute mAP.
2022-03-07 22:21:09,052 val mAP=0.584589.
2022-03-07 22:21:09,053 the monitor loses its patience to 9!.
2022-03-07 22:21:30,229 epoch 19: avg loss=4.276662, avg quantization error=0.015376.
2022-03-07 22:21:30,230 begin to evaluate model.
2022-03-07 22:22:46,526 compute mAP.
2022-03-07 22:23:03,882 val mAP=0.585504.
2022-03-07 22:23:03,884 the monitor loses its patience to 8!.
2022-03-07 22:23:24,980 epoch 20: avg loss=4.290248, avg quantization error=0.015230.
2022-03-07 22:23:24,980 begin to evaluate model.
2022-03-07 22:24:42,969 compute mAP.
2022-03-07 22:25:00,102 val mAP=0.583084.
2022-03-07 22:25:00,103 the monitor loses its patience to 7!.
2022-03-07 22:25:21,568 epoch 21: avg loss=4.291010, avg quantization error=0.015300.
2022-03-07 22:25:21,568 begin to evaluate model.
2022-03-07 22:26:38,229 compute mAP.
2022-03-07 22:26:55,682 val mAP=0.583737.
2022-03-07 22:26:55,683 the monitor loses its patience to 6!.
2022-03-07 22:27:17,367 epoch 22: avg loss=4.280690, avg quantization error=0.015536.
2022-03-07 22:27:17,368 begin to evaluate model.
2022-03-07 22:28:34,652 compute mAP.
2022-03-07 22:28:52,447 val mAP=0.582594.
2022-03-07 22:28:52,449 the monitor loses its patience to 5!.
2022-03-07 22:29:12,949 epoch 23: avg loss=4.256904, avg quantization error=0.015340.
2022-03-07 22:29:12,950 begin to evaluate model.
2022-03-07 22:30:28,692 compute mAP.
2022-03-07 22:30:46,095 val mAP=0.584053.
2022-03-07 22:30:46,096 the monitor loses its patience to 4!.
2022-03-07 22:31:08,213 epoch 24: avg loss=4.248620, avg quantization error=0.015351.
2022-03-07 22:31:08,213 begin to evaluate model.
2022-03-07 22:32:24,754 compute mAP.
2022-03-07 22:32:41,956 val mAP=0.588205.
2022-03-07 22:32:41,957 the monitor loses its patience to 3!.
2022-03-07 22:33:02,947 epoch 25: avg loss=4.240185, avg quantization error=0.015329.
2022-03-07 22:33:02,947 begin to evaluate model.
2022-03-07 22:34:18,545 compute mAP.
2022-03-07 22:34:36,086 val mAP=0.591356.
2022-03-07 22:34:36,088 save the best model, db_codes and db_targets.
2022-03-07 22:34:38,943 finish saving.
2022-03-07 22:35:00,270 epoch 26: avg loss=4.252956, avg quantization error=0.015325.
2022-03-07 22:35:00,271 begin to evaluate model.
2022-03-07 22:36:15,992 compute mAP.
2022-03-07 22:36:33,145 val mAP=0.589814.
2022-03-07 22:36:33,146 the monitor loses its patience to 9!.
2022-03-07 22:36:54,117 epoch 27: avg loss=4.225904, avg quantization error=0.015411.
2022-03-07 22:36:54,118 begin to evaluate model.
2022-03-07 22:38:09,950 compute mAP.
2022-03-07 22:38:27,044 val mAP=0.592761.
2022-03-07 22:38:27,045 save the best model, db_codes and db_targets.
2022-03-07 22:38:29,569 finish saving.
2022-03-07 22:38:49,852 epoch 28: avg loss=4.222061, avg quantization error=0.015400.
2022-03-07 22:38:49,852 begin to evaluate model.
2022-03-07 22:40:04,777 compute mAP.
2022-03-07 22:40:21,972 val mAP=0.591419.
2022-03-07 22:40:21,973 the monitor loses its patience to 9!.
2022-03-07 22:40:44,785 epoch 29: avg loss=4.217087, avg quantization error=0.015321.
2022-03-07 22:40:44,786 begin to evaluate model.
2022-03-07 22:42:00,643 compute mAP.
2022-03-07 22:42:17,448 val mAP=0.593866.
2022-03-07 22:42:17,449 save the best model, db_codes and db_targets.
2022-03-07 22:42:20,072 finish saving.
2022-03-07 22:42:41,433 epoch 30: avg loss=4.220634, avg quantization error=0.015380.
2022-03-07 22:42:41,434 begin to evaluate model.
2022-03-07 22:43:58,048 compute mAP.
2022-03-07 22:44:15,662 val mAP=0.595082.
2022-03-07 22:44:15,663 save the best model, db_codes and db_targets.
2022-03-07 22:44:18,215 finish saving.
2022-03-07 22:44:40,029 epoch 31: avg loss=4.207351, avg quantization error=0.015316.
2022-03-07 22:44:40,029 begin to evaluate model.
2022-03-07 22:45:56,102 compute mAP.
2022-03-07 22:46:12,999 val mAP=0.597767.
2022-03-07 22:46:13,000 save the best model, db_codes and db_targets.
2022-03-07 22:46:17,978 finish saving.
2022-03-07 22:46:39,739 epoch 32: avg loss=4.222591, avg quantization error=0.015326.
2022-03-07 22:46:39,739 begin to evaluate model.
2022-03-07 22:47:55,773 compute mAP.
2022-03-07 22:48:13,007 val mAP=0.597515.
2022-03-07 22:48:13,007 the monitor loses its patience to 9!.
2022-03-07 22:48:34,551 epoch 33: avg loss=4.193303, avg quantization error=0.015375.
2022-03-07 22:48:34,551 begin to evaluate model.
2022-03-07 22:49:50,843 compute mAP.
2022-03-07 22:50:08,724 val mAP=0.596502.
2022-03-07 22:50:08,724 the monitor loses its patience to 8!.
2022-03-07 22:50:30,085 epoch 34: avg loss=4.196375, avg quantization error=0.015351.
2022-03-07 22:50:30,086 begin to evaluate model.
2022-03-07 22:51:46,106 compute mAP.
2022-03-07 22:52:03,334 val mAP=0.595729.
2022-03-07 22:52:03,335 the monitor loses its patience to 7!.
2022-03-07 22:52:25,124 epoch 35: avg loss=4.193597, avg quantization error=0.015243.
2022-03-07 22:52:25,125 begin to evaluate model.
2022-03-07 22:53:40,193 compute mAP.
2022-03-07 22:53:58,079 val mAP=0.593535.
2022-03-07 22:53:58,080 the monitor loses its patience to 6!.
2022-03-07 22:54:19,686 epoch 36: avg loss=4.185015, avg quantization error=0.015410.
2022-03-07 22:54:19,687 begin to evaluate model.
2022-03-07 22:55:35,665 compute mAP.
2022-03-07 22:55:53,153 val mAP=0.597001.
2022-03-07 22:55:53,154 the monitor loses its patience to 5!.
2022-03-07 22:56:15,408 epoch 37: avg loss=4.175928, avg quantization error=0.015364.
2022-03-07 22:56:15,408 begin to evaluate model.
2022-03-07 22:57:31,919 compute mAP.
2022-03-07 22:57:48,981 val mAP=0.595822.
2022-03-07 22:57:48,981 the monitor loses its patience to 4!.
2022-03-07 22:58:10,015 epoch 38: avg loss=4.186193, avg quantization error=0.015372.
2022-03-07 22:58:10,016 begin to evaluate model.
2022-03-07 22:59:25,936 compute mAP.
2022-03-07 22:59:43,242 val mAP=0.597913.
2022-03-07 22:59:43,243 save the best model, db_codes and db_targets.
2022-03-07 22:59:45,806 finish saving.
2022-03-07 23:00:06,761 epoch 39: avg loss=4.165717, avg quantization error=0.015352.
2022-03-07 23:00:06,762 begin to evaluate model.
2022-03-07 23:01:24,140 compute mAP.
2022-03-07 23:01:41,682 val mAP=0.599393.
2022-03-07 23:01:41,683 save the best model, db_codes and db_targets.
2022-03-07 23:01:44,363 finish saving.
2022-03-07 23:02:05,685 epoch 40: avg loss=4.155042, avg quantization error=0.015343.
2022-03-07 23:02:05,686 begin to evaluate model.
2022-03-07 23:03:20,966 compute mAP.
2022-03-07 23:03:38,250 val mAP=0.598616.
2022-03-07 23:03:38,250 the monitor loses its patience to 9!.
2022-03-07 23:03:59,259 epoch 41: avg loss=4.172553, avg quantization error=0.015343.
2022-03-07 23:03:59,260 begin to evaluate model.
2022-03-07 23:05:15,519 compute mAP.
2022-03-07 23:05:32,828 val mAP=0.598927.
2022-03-07 23:05:32,828 the monitor loses its patience to 8!.
2022-03-07 23:05:53,408 epoch 42: avg loss=4.163091, avg quantization error=0.015340.
2022-03-07 23:05:53,408 begin to evaluate model.
2022-03-07 23:07:10,128 compute mAP.
2022-03-07 23:07:27,208 val mAP=0.599551.
2022-03-07 23:07:27,209 save the best model, db_codes and db_targets.
2022-03-07 23:07:29,871 finish saving.
2022-03-07 23:07:50,895 epoch 43: avg loss=4.148011, avg quantization error=0.015330.
2022-03-07 23:07:50,895 begin to evaluate model.
2022-03-07 23:09:07,640 compute mAP.
2022-03-07 23:09:24,572 val mAP=0.600545.
2022-03-07 23:09:24,573 save the best model, db_codes and db_targets.
2022-03-07 23:09:27,221 finish saving.
2022-03-07 23:09:48,576 epoch 44: avg loss=4.168090, avg quantization error=0.015334.
2022-03-07 23:09:48,576 begin to evaluate model.
2022-03-07 23:11:05,477 compute mAP.
2022-03-07 23:11:22,470 val mAP=0.600667.
2022-03-07 23:11:22,471 save the best model, db_codes and db_targets.
2022-03-07 23:11:25,861 finish saving.
2022-03-07 23:11:46,814 epoch 45: avg loss=4.148290, avg quantization error=0.015296.
2022-03-07 23:11:46,815 begin to evaluate model.
2022-03-07 23:13:02,465 compute mAP.
2022-03-07 23:13:19,296 val mAP=0.599962.
2022-03-07 23:13:19,297 the monitor loses its patience to 9!.
2022-03-07 23:13:40,332 epoch 46: avg loss=4.149807, avg quantization error=0.015343.
2022-03-07 23:13:40,333 begin to evaluate model.
2022-03-07 23:14:56,712 compute mAP.
2022-03-07 23:15:14,196 val mAP=0.600148.
2022-03-07 23:15:14,196 the monitor loses its patience to 8!.
2022-03-07 23:15:34,486 epoch 47: avg loss=4.160675, avg quantization error=0.015311.
2022-03-07 23:15:34,487 begin to evaluate model.
2022-03-07 23:16:49,795 compute mAP.
2022-03-07 23:17:07,443 val mAP=0.600741.
2022-03-07 23:17:07,444 save the best model, db_codes and db_targets.
2022-03-07 23:17:10,235 finish saving.
2022-03-07 23:17:30,979 epoch 48: avg loss=4.168533, avg quantization error=0.015336.
2022-03-07 23:17:30,979 begin to evaluate model.
2022-03-07 23:18:46,736 compute mAP.
2022-03-07 23:19:03,897 val mAP=0.600459.
2022-03-07 23:19:03,898 the monitor loses its patience to 9!.
2022-03-07 23:19:24,616 epoch 49: avg loss=4.155617, avg quantization error=0.015310.
2022-03-07 23:19:24,617 begin to evaluate model.
2022-03-07 23:20:40,263 compute mAP.
2022-03-07 23:20:57,437 val mAP=0.600525.
2022-03-07 23:20:57,438 the monitor loses its patience to 8!.
2022-03-07 23:20:57,438 free the queue memory.
2022-03-07 23:20:57,439 finish trainning at epoch 49.
2022-03-07 23:20:57,441 finish training, now load the best model and codes.
2022-03-07 23:20:58,501 begin to test model.
2022-03-07 23:20:58,501 compute mAP.
2022-03-07 23:21:15,614 test mAP=0.600741.
2022-03-07 23:21:15,614 compute PR curve and P@top1000 curve.
2022-03-07 23:21:50,200 finish testing.
2022-03-07 23:21:50,201 finish all procedures.
