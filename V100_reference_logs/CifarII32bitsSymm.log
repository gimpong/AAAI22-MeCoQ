2022-03-07 21:45:57,903 config: Namespace(K=256, M=4, T=0.35, alpha=10, batch_size=128, checkpoint_root='./checkpoints/CifarII32bitsSymm', dataset='CIFAR10', device='cuda:0', download_cifar10=False, epoch_num=50, eval_interval=1, feat_dim=48, final_lr=1e-05, hp_beta=0.005, hp_gamma=0.5, hp_lambda=0.1, is_asym_dist=False, lr=0.01, lr_scaling=0.001, mode='debias', momentum=0.9, monitor_counter=10, notes='CifarII32bitsSymm', num_workers=20, optimizer='SGD', pos_prior=0.1, protocal='II', queue_begin_epoch=15, seed=2021, start_lr=1e-05, topK=1000, trainable_layer_num=2, use_scheduler=True, use_writer=True, vgg_model_path='vgg16.pth', warmup_epoch_num=1).
2022-03-07 21:45:57,903 prepare CIFAR10 datatset.
2022-03-07 21:45:59,255 setup model.
2022-03-07 21:47:55,701 config: Namespace(K=256, M=4, T=0.35, alpha=10, batch_size=128, checkpoint_root='./checkpoints/CifarII32bitsSymm', dataset='CIFAR10', device='cuda:0', download_cifar10=False, epoch_num=50, eval_interval=1, feat_dim=48, final_lr=1e-05, hp_beta=0.005, hp_gamma=0.5, hp_lambda=0.1, is_asym_dist=False, lr=0.01, lr_scaling=0.001, mode='debias', momentum=0.9, monitor_counter=10, notes='CifarII32bitsSymm', num_workers=20, optimizer='SGD', pos_prior=0.1, protocal='II', queue_begin_epoch=15, seed=2021, start_lr=1e-05, topK=1000, trainable_layer_num=2, use_scheduler=True, use_writer=True, vgg_model_path='vgg16.pth', warmup_epoch_num=1).
2022-03-07 21:47:55,718 prepare CIFAR10 datatset.
2022-03-07 21:47:57,025 setup model.
2022-03-07 21:49:31,508 config: Namespace(K=256, M=4, T=0.35, alpha=10, batch_size=128, checkpoint_root='./checkpoints/CifarII32bitsSymm', dataset='CIFAR10', device='cuda:0', download_cifar10=False, epoch_num=50, eval_interval=1, feat_dim=48, final_lr=1e-05, hp_beta=0.005, hp_gamma=0.5, hp_lambda=0.1, is_asym_dist=False, lr=0.01, lr_scaling=0.001, mode='debias', momentum=0.9, monitor_counter=10, notes='CifarII32bitsSymm', num_workers=20, optimizer='SGD', pos_prior=0.1, protocal='II', queue_begin_epoch=15, seed=2021, start_lr=1e-05, topK=1000, trainable_layer_num=2, use_scheduler=True, use_writer=True, vgg_model_path='vgg16.pth', warmup_epoch_num=1).
2022-03-07 21:49:31,530 prepare CIFAR10 datatset.
2022-03-07 21:49:33,309 setup model.
2022-03-07 21:49:41,846 define loss function.
2022-03-07 21:49:41,847 setup SGD optimizer.
2022-03-07 21:49:41,848 prepare monitor and evaluator.
2022-03-07 21:49:41,848 begin to train model.
2022-03-07 21:49:41,849 register queue.
2022-03-07 21:50:02,098 epoch 0: avg loss=4.437604, avg quantization error=0.018130.
2022-03-07 21:50:02,098 begin to evaluate model.
2022-03-07 21:51:17,953 compute mAP.
2022-03-07 21:51:35,238 val mAP=0.486780.
2022-03-07 21:51:35,239 save the best model, db_codes and db_targets.
2022-03-07 21:51:42,172 finish saving.
2022-03-07 21:52:02,669 epoch 1: avg loss=3.332088, avg quantization error=0.016031.
2022-03-07 21:52:02,670 begin to evaluate model.
2022-03-07 21:53:19,021 compute mAP.
2022-03-07 21:53:36,483 val mAP=0.528423.
2022-03-07 21:53:36,484 save the best model, db_codes and db_targets.
2022-03-07 21:53:39,049 finish saving.
2022-03-07 21:54:00,414 epoch 2: avg loss=3.033501, avg quantization error=0.015483.
2022-03-07 21:54:00,415 begin to evaluate model.
2022-03-07 21:55:17,453 compute mAP.
2022-03-07 21:55:35,188 val mAP=0.528357.
2022-03-07 21:55:35,188 the monitor loses its patience to 9!.
2022-03-07 21:55:56,496 epoch 3: avg loss=2.883157, avg quantization error=0.015127.
2022-03-07 21:55:56,496 begin to evaluate model.
2022-03-07 21:57:13,179 compute mAP.
2022-03-07 21:57:30,220 val mAP=0.552208.
2022-03-07 21:57:30,220 save the best model, db_codes and db_targets.
2022-03-07 21:57:32,717 finish saving.
2022-03-07 21:57:53,934 epoch 4: avg loss=2.809629, avg quantization error=0.015076.
2022-03-07 21:57:53,934 begin to evaluate model.
2022-03-07 21:59:09,217 compute mAP.
2022-03-07 21:59:27,037 val mAP=0.564167.
2022-03-07 21:59:27,037 save the best model, db_codes and db_targets.
2022-03-07 21:59:29,553 finish saving.
2022-03-07 21:59:51,624 epoch 5: avg loss=2.714674, avg quantization error=0.015036.
2022-03-07 21:59:51,624 begin to evaluate model.
2022-03-07 22:01:07,591 compute mAP.
2022-03-07 22:01:25,221 val mAP=0.576450.
2022-03-07 22:01:25,222 save the best model, db_codes and db_targets.
2022-03-07 22:01:27,760 finish saving.
2022-03-07 22:01:49,091 epoch 6: avg loss=2.608639, avg quantization error=0.015044.
2022-03-07 22:01:49,091 begin to evaluate model.
2022-03-07 22:03:04,382 compute mAP.
2022-03-07 22:03:21,714 val mAP=0.577763.
2022-03-07 22:03:21,714 save the best model, db_codes and db_targets.
2022-03-07 22:03:24,367 finish saving.
2022-03-07 22:03:45,165 epoch 7: avg loss=2.573084, avg quantization error=0.015059.
2022-03-07 22:03:45,166 begin to evaluate model.
2022-03-07 22:05:01,240 compute mAP.
2022-03-07 22:05:18,167 val mAP=0.585346.
2022-03-07 22:05:18,168 save the best model, db_codes and db_targets.
2022-03-07 22:05:20,773 finish saving.
2022-03-07 22:05:41,807 epoch 8: avg loss=2.469836, avg quantization error=0.014957.
2022-03-07 22:05:41,808 begin to evaluate model.
2022-03-07 22:06:58,265 compute mAP.
2022-03-07 22:07:15,456 val mAP=0.583662.
2022-03-07 22:07:15,457 the monitor loses its patience to 9!.
2022-03-07 22:07:35,988 epoch 9: avg loss=2.452103, avg quantization error=0.015042.
2022-03-07 22:07:35,989 begin to evaluate model.
2022-03-07 22:08:52,829 compute mAP.
2022-03-07 22:09:10,421 val mAP=0.593560.
2022-03-07 22:09:10,422 save the best model, db_codes and db_targets.
2022-03-07 22:09:12,895 finish saving.
2022-03-07 22:09:34,132 epoch 10: avg loss=2.416968, avg quantization error=0.014976.
2022-03-07 22:09:34,133 begin to evaluate model.
2022-03-07 22:10:51,237 compute mAP.
2022-03-07 22:11:08,624 val mAP=0.598663.
2022-03-07 22:11:08,624 save the best model, db_codes and db_targets.
2022-03-07 22:11:11,135 finish saving.
2022-03-07 22:11:32,358 epoch 11: avg loss=2.341450, avg quantization error=0.015015.
2022-03-07 22:11:32,358 begin to evaluate model.
2022-03-07 22:12:47,522 compute mAP.
2022-03-07 22:13:05,047 val mAP=0.599239.
2022-03-07 22:13:05,048 save the best model, db_codes and db_targets.
2022-03-07 22:13:07,570 finish saving.
2022-03-07 22:13:28,489 epoch 12: avg loss=2.323458, avg quantization error=0.015073.
2022-03-07 22:13:28,489 begin to evaluate model.
2022-03-07 22:14:44,223 compute mAP.
2022-03-07 22:15:01,206 val mAP=0.600557.
2022-03-07 22:15:01,206 save the best model, db_codes and db_targets.
2022-03-07 22:15:04,007 finish saving.
2022-03-07 22:15:24,862 epoch 13: avg loss=2.278204, avg quantization error=0.015084.
2022-03-07 22:15:24,863 begin to evaluate model.
2022-03-07 22:16:41,748 compute mAP.
2022-03-07 22:16:58,811 val mAP=0.600380.
2022-03-07 22:16:58,811 the monitor loses its patience to 9!.
2022-03-07 22:17:19,873 epoch 14: avg loss=2.162705, avg quantization error=0.015017.
2022-03-07 22:17:19,874 begin to evaluate model.
2022-03-07 22:18:36,366 compute mAP.
2022-03-07 22:18:54,519 val mAP=0.607620.
2022-03-07 22:18:54,520 save the best model, db_codes and db_targets.
2022-03-07 22:18:57,059 finish saving.
2022-03-07 22:19:18,505 epoch 15: avg loss=4.925525, avg quantization error=0.015168.
2022-03-07 22:19:18,506 begin to evaluate model.
2022-03-07 22:20:35,343 compute mAP.
2022-03-07 22:20:53,141 val mAP=0.608541.
2022-03-07 22:20:53,142 save the best model, db_codes and db_targets.
2022-03-07 22:20:55,748 finish saving.
2022-03-07 22:21:16,424 epoch 16: avg loss=4.886516, avg quantization error=0.015195.
2022-03-07 22:21:16,425 begin to evaluate model.
2022-03-07 22:22:32,823 compute mAP.
2022-03-07 22:22:50,629 val mAP=0.607523.
2022-03-07 22:22:50,629 the monitor loses its patience to 9!.
2022-03-07 22:23:11,487 epoch 17: avg loss=4.864186, avg quantization error=0.015134.
2022-03-07 22:23:11,487 begin to evaluate model.
2022-03-07 22:24:28,805 compute mAP.
2022-03-07 22:24:46,416 val mAP=0.607532.
2022-03-07 22:24:46,417 the monitor loses its patience to 8!.
2022-03-07 22:25:07,043 epoch 18: avg loss=4.851393, avg quantization error=0.015098.
2022-03-07 22:25:07,043 begin to evaluate model.
2022-03-07 22:26:23,565 compute mAP.
2022-03-07 22:26:40,849 val mAP=0.607322.
2022-03-07 22:26:40,850 the monitor loses its patience to 7!.
2022-03-07 22:27:02,085 epoch 19: avg loss=4.832623, avg quantization error=0.015025.
2022-03-07 22:27:02,086 begin to evaluate model.
2022-03-07 22:28:19,284 compute mAP.
2022-03-07 22:28:36,957 val mAP=0.606306.
2022-03-07 22:28:36,958 the monitor loses its patience to 6!.
2022-03-07 22:28:57,551 epoch 20: avg loss=4.818027, avg quantization error=0.015072.
2022-03-07 22:28:57,551 begin to evaluate model.
2022-03-07 22:30:13,699 compute mAP.
2022-03-07 22:30:31,298 val mAP=0.606721.
2022-03-07 22:30:31,300 the monitor loses its patience to 5!.
2022-03-07 22:30:52,313 epoch 21: avg loss=4.821543, avg quantization error=0.014998.
2022-03-07 22:30:52,314 begin to evaluate model.
2022-03-07 22:32:08,340 compute mAP.
2022-03-07 22:32:25,325 val mAP=0.606742.
2022-03-07 22:32:25,325 the monitor loses its patience to 4!.
2022-03-07 22:32:46,476 epoch 22: avg loss=4.817443, avg quantization error=0.014949.
2022-03-07 22:32:46,476 begin to evaluate model.
2022-03-07 22:34:03,241 compute mAP.
2022-03-07 22:34:20,717 val mAP=0.608177.
2022-03-07 22:34:20,718 the monitor loses its patience to 3!.
2022-03-07 22:34:41,849 epoch 23: avg loss=4.814710, avg quantization error=0.014897.
2022-03-07 22:34:41,850 begin to evaluate model.
2022-03-07 22:35:57,186 compute mAP.
2022-03-07 22:36:14,167 val mAP=0.609440.
2022-03-07 22:36:14,168 save the best model, db_codes and db_targets.
2022-03-07 22:36:16,878 finish saving.
2022-03-07 22:36:38,191 epoch 24: avg loss=4.798827, avg quantization error=0.014856.
2022-03-07 22:36:38,191 begin to evaluate model.
2022-03-07 22:37:53,724 compute mAP.
2022-03-07 22:38:11,113 val mAP=0.608476.
2022-03-07 22:38:11,114 the monitor loses its patience to 9!.
2022-03-07 22:38:32,554 epoch 25: avg loss=4.790356, avg quantization error=0.014781.
2022-03-07 22:38:32,555 begin to evaluate model.
2022-03-07 22:39:49,356 compute mAP.
2022-03-07 22:40:06,820 val mAP=0.608190.
2022-03-07 22:40:06,821 the monitor loses its patience to 8!.
2022-03-07 22:40:28,054 epoch 26: avg loss=4.799367, avg quantization error=0.014824.
2022-03-07 22:40:28,054 begin to evaluate model.
2022-03-07 22:41:45,405 compute mAP.
2022-03-07 22:42:02,595 val mAP=0.608303.
2022-03-07 22:42:02,596 the monitor loses its patience to 7!.
2022-03-07 22:42:23,818 epoch 27: avg loss=4.771553, avg quantization error=0.014817.
2022-03-07 22:42:23,819 begin to evaluate model.
2022-03-07 22:43:40,398 compute mAP.
2022-03-07 22:43:57,286 val mAP=0.609333.
2022-03-07 22:43:57,286 the monitor loses its patience to 6!.
2022-03-07 22:44:18,779 epoch 28: avg loss=4.772174, avg quantization error=0.014782.
2022-03-07 22:44:18,780 begin to evaluate model.
2022-03-07 22:45:35,698 compute mAP.
2022-03-07 22:45:52,987 val mAP=0.608838.
2022-03-07 22:45:52,987 the monitor loses its patience to 5!.
2022-03-07 22:46:13,771 epoch 29: avg loss=4.761080, avg quantization error=0.014807.
2022-03-07 22:46:13,771 begin to evaluate model.
2022-03-07 22:47:30,397 compute mAP.
2022-03-07 22:47:47,206 val mAP=0.610111.
2022-03-07 22:47:47,207 save the best model, db_codes and db_targets.
2022-03-07 22:47:49,724 finish saving.
2022-03-07 22:48:10,408 epoch 30: avg loss=4.764152, avg quantization error=0.014718.
2022-03-07 22:48:10,409 begin to evaluate model.
2022-03-07 22:49:27,523 compute mAP.
2022-03-07 22:49:44,790 val mAP=0.611754.
2022-03-07 22:49:44,791 save the best model, db_codes and db_targets.
2022-03-07 22:49:47,503 finish saving.
2022-03-07 22:50:08,607 epoch 31: avg loss=4.756757, avg quantization error=0.014675.
2022-03-07 22:50:08,608 begin to evaluate model.
2022-03-07 22:51:24,565 compute mAP.
2022-03-07 22:51:41,845 val mAP=0.611447.
2022-03-07 22:51:41,846 the monitor loses its patience to 9!.
2022-03-07 22:52:02,950 epoch 32: avg loss=4.760435, avg quantization error=0.014708.
2022-03-07 22:52:02,950 begin to evaluate model.
2022-03-07 22:53:19,232 compute mAP.
2022-03-07 22:53:36,031 val mAP=0.612677.
2022-03-07 22:53:36,032 save the best model, db_codes and db_targets.
2022-03-07 22:53:38,662 finish saving.
2022-03-07 22:53:59,606 epoch 33: avg loss=4.745976, avg quantization error=0.014685.
2022-03-07 22:53:59,606 begin to evaluate model.
2022-03-07 22:55:15,988 compute mAP.
2022-03-07 22:55:33,571 val mAP=0.613989.
2022-03-07 22:55:33,572 save the best model, db_codes and db_targets.
2022-03-07 22:55:36,202 finish saving.
2022-03-07 22:55:57,230 epoch 34: avg loss=4.747023, avg quantization error=0.014713.
2022-03-07 22:55:57,231 begin to evaluate model.
2022-03-07 22:57:12,902 compute mAP.
2022-03-07 22:57:29,941 val mAP=0.614763.
2022-03-07 22:57:29,942 save the best model, db_codes and db_targets.
2022-03-07 22:57:44,887 finish saving.
2022-03-07 22:58:06,325 epoch 35: avg loss=4.749080, avg quantization error=0.014635.
2022-03-07 22:58:06,326 begin to evaluate model.
2022-03-07 22:59:23,072 compute mAP.
2022-03-07 22:59:41,065 val mAP=0.613639.
2022-03-07 22:59:41,065 the monitor loses its patience to 9!.
2022-03-07 23:00:01,602 epoch 36: avg loss=4.744280, avg quantization error=0.014648.
2022-03-07 23:00:01,603 begin to evaluate model.
2022-03-07 23:01:18,980 compute mAP.
2022-03-07 23:01:36,744 val mAP=0.613116.
2022-03-07 23:01:36,745 the monitor loses its patience to 8!.
2022-03-07 23:01:58,135 epoch 37: avg loss=4.726515, avg quantization error=0.014586.
2022-03-07 23:01:58,135 begin to evaluate model.
2022-03-07 23:03:15,245 compute mAP.
2022-03-07 23:03:33,095 val mAP=0.613387.
2022-03-07 23:03:33,096 the monitor loses its patience to 7!.
2022-03-07 23:03:54,322 epoch 38: avg loss=4.738370, avg quantization error=0.014611.
2022-03-07 23:03:54,323 begin to evaluate model.
2022-03-07 23:05:10,833 compute mAP.
2022-03-07 23:05:27,653 val mAP=0.613893.
2022-03-07 23:05:27,654 the monitor loses its patience to 6!.
2022-03-07 23:05:49,079 epoch 39: avg loss=4.730445, avg quantization error=0.014656.
2022-03-07 23:05:49,080 begin to evaluate model.
2022-03-07 23:07:05,756 compute mAP.
2022-03-07 23:07:23,088 val mAP=0.614221.
2022-03-07 23:07:23,089 the monitor loses its patience to 5!.
2022-03-07 23:07:44,132 epoch 40: avg loss=4.726323, avg quantization error=0.014609.
2022-03-07 23:07:44,132 begin to evaluate model.
2022-03-07 23:09:00,282 compute mAP.
2022-03-07 23:09:17,864 val mAP=0.614540.
2022-03-07 23:09:17,864 the monitor loses its patience to 4!.
2022-03-07 23:09:38,736 epoch 41: avg loss=4.736302, avg quantization error=0.014617.
2022-03-07 23:09:38,736 begin to evaluate model.
2022-03-07 23:10:55,676 compute mAP.
2022-03-07 23:11:12,695 val mAP=0.614227.
2022-03-07 23:11:12,696 the monitor loses its patience to 3!.
2022-03-07 23:11:34,156 epoch 42: avg loss=4.734704, avg quantization error=0.014595.
2022-03-07 23:11:34,157 begin to evaluate model.
2022-03-07 23:12:50,398 compute mAP.
2022-03-07 23:13:07,968 val mAP=0.613856.
2022-03-07 23:13:07,969 the monitor loses its patience to 2!.
2022-03-07 23:13:29,050 epoch 43: avg loss=4.731871, avg quantization error=0.014612.
2022-03-07 23:13:29,050 begin to evaluate model.
2022-03-07 23:14:45,141 compute mAP.
2022-03-07 23:15:01,891 val mAP=0.613556.
2022-03-07 23:15:01,892 the monitor loses its patience to 1!.
2022-03-07 23:15:23,581 epoch 44: avg loss=4.722985, avg quantization error=0.014568.
2022-03-07 23:15:23,582 begin to evaluate model.
2022-03-07 23:16:40,612 compute mAP.
2022-03-07 23:16:58,107 val mAP=0.613450.
2022-03-07 23:16:58,108 the monitor loses its patience to 0!.
2022-03-07 23:16:58,108 early stop.
2022-03-07 23:16:58,109 free the queue memory.
2022-03-07 23:16:58,109 finish trainning at epoch 44.
2022-03-07 23:16:58,112 finish training, now load the best model and codes.
2022-03-07 23:16:59,093 begin to test model.
2022-03-07 23:16:59,093 compute mAP.
2022-03-07 23:17:16,348 test mAP=0.614763.
2022-03-07 23:17:16,349 compute PR curve and P@top1000 curve.
2022-03-07 23:17:51,604 finish testing.
2022-03-07 23:17:51,604 finish all procedures.
