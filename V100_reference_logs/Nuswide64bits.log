2022-03-07 21:48:17,504 config: Namespace(K=256, M=8, T=0.4, alpha=10, batch_size=128, checkpoint_root='./checkpoints/Nuswide64bits', dataset='NUSWIDE', device='cuda:0', download_cifar10=False, epoch_num=50, eval_interval=1, feat_dim=128, final_lr=1e-05, hp_beta=0.01, hp_gamma=0.5, hp_lambda=0.01, is_asym_dist=True, lr=0.01, lr_scaling=0.001, mode='debias', momentum=0.9, monitor_counter=10, notes='Nuswide64bits', num_workers=20, optimizer='SGD', pos_prior=0.15, protocal='I', queue_begin_epoch=10, seed=2021, start_lr=1e-05, topK=5000, trainable_layer_num=0, use_scheduler=True, use_writer=True, vgg_model_path='vgg16.pth', warmup_epoch_num=1).
2022-03-07 21:48:17,505 prepare NUSWIDE datatset.
2022-03-07 21:48:31,118 setup model.
2022-03-07 21:48:38,632 define loss function.
2022-03-07 21:48:38,632 setup SGD optimizer.
2022-03-07 21:48:38,634 prepare monitor and evaluator.
2022-03-07 21:48:38,637 begin to train model.
2022-03-07 21:48:38,638 register queue.
2022-03-07 22:45:56,805 epoch 0: avg loss=1.657673, avg quantization error=0.017062.
2022-03-07 22:45:56,805 begin to evaluate model.
2022-03-07 22:50:57,502 compute mAP.
2022-03-07 22:51:39,914 val mAP=0.823107.
2022-03-07 22:51:39,915 save the best model, db_codes and db_targets.
2022-03-07 22:51:42,833 finish saving.
2022-03-07 23:04:08,490 epoch 1: avg loss=1.057578, avg quantization error=0.017765.
2022-03-07 23:04:08,490 begin to evaluate model.
2022-03-07 23:09:00,168 compute mAP.
2022-03-07 23:09:06,790 val mAP=0.825113.
2022-03-07 23:09:06,790 save the best model, db_codes and db_targets.
2022-03-07 23:09:09,592 finish saving.
2022-03-07 23:21:29,232 epoch 2: avg loss=1.031419, avg quantization error=0.018154.
2022-03-07 23:21:29,232 begin to evaluate model.
2022-03-07 23:26:22,377 compute mAP.
2022-03-07 23:27:05,209 val mAP=0.824191.
2022-03-07 23:27:05,210 the monitor loses its patience to 9!.
2022-03-07 23:40:53,979 epoch 3: avg loss=1.018622, avg quantization error=0.018325.
2022-03-07 23:40:53,979 begin to evaluate model.
2022-03-07 23:45:50,526 compute mAP.
2022-03-07 23:46:36,434 val mAP=0.825249.
2022-03-07 23:46:36,435 save the best model, db_codes and db_targets.
2022-03-07 23:46:39,527 finish saving.
2022-03-07 23:59:31,590 epoch 4: avg loss=1.010298, avg quantization error=0.018543.
2022-03-07 23:59:31,590 begin to evaluate model.
2022-03-08 00:05:27,658 compute mAP.
2022-03-08 00:06:13,302 val mAP=0.824344.
2022-03-08 00:06:13,303 the monitor loses its patience to 9!.
2022-03-08 00:20:24,116 epoch 5: avg loss=1.006019, avg quantization error=0.018713.
2022-03-08 00:20:24,116 begin to evaluate model.
2022-03-08 00:26:17,833 compute mAP.
2022-03-08 00:27:03,426 val mAP=0.825214.
2022-03-08 00:27:03,427 the monitor loses its patience to 8!.
2022-03-08 00:43:15,338 epoch 6: avg loss=1.002046, avg quantization error=0.018873.
2022-03-08 00:43:15,338 begin to evaluate model.
2022-03-08 00:49:31,140 compute mAP.
2022-03-08 00:50:19,453 val mAP=0.825419.
2022-03-08 00:50:19,453 save the best model, db_codes and db_targets.
2022-03-08 00:50:22,438 finish saving.
2022-03-08 01:02:37,777 epoch 7: avg loss=0.998256, avg quantization error=0.018962.
2022-03-08 01:02:37,777 begin to evaluate model.
2022-03-08 01:07:29,002 compute mAP.
2022-03-08 01:08:15,076 val mAP=0.825513.
2022-03-08 01:08:15,077 save the best model, db_codes and db_targets.
2022-03-08 01:08:18,153 finish saving.
2022-03-08 01:21:06,970 epoch 8: avg loss=0.995779, avg quantization error=0.018991.
2022-03-08 01:21:06,970 begin to evaluate model.
2022-03-08 01:26:25,624 compute mAP.
2022-03-08 01:27:03,246 val mAP=0.825663.
2022-03-08 01:27:03,247 save the best model, db_codes and db_targets.
2022-03-08 01:27:06,045 finish saving.
2022-03-08 01:40:17,946 epoch 9: avg loss=0.999432, avg quantization error=0.019099.
2022-03-08 01:40:17,947 begin to evaluate model.
2022-03-08 01:45:14,843 compute mAP.
2022-03-08 01:46:01,583 val mAP=0.823781.
2022-03-08 01:46:01,584 the monitor loses its patience to 9!.
2022-03-08 02:02:16,065 epoch 10: avg loss=4.626227, avg quantization error=0.018831.
2022-03-08 02:02:16,065 begin to evaluate model.
2022-03-08 02:07:31,338 compute mAP.
2022-03-08 02:08:17,591 val mAP=0.827840.
2022-03-08 02:08:17,592 save the best model, db_codes and db_targets.
2022-03-08 02:08:20,503 finish saving.
2022-03-08 02:21:53,369 epoch 11: avg loss=4.619783, avg quantization error=0.018699.
2022-03-08 02:21:53,369 begin to evaluate model.
2022-03-08 02:26:47,016 compute mAP.
2022-03-08 02:27:30,995 val mAP=0.827947.
2022-03-08 02:27:30,996 save the best model, db_codes and db_targets.
2022-03-08 02:27:33,898 finish saving.
2022-03-08 02:41:27,196 epoch 12: avg loss=4.612933, avg quantization error=0.018794.
2022-03-08 02:41:27,196 begin to evaluate model.
2022-03-08 02:47:36,410 compute mAP.
2022-03-08 02:48:28,604 val mAP=0.827970.
2022-03-08 02:48:28,605 save the best model, db_codes and db_targets.
2022-03-08 02:48:31,533 finish saving.
2022-03-08 03:03:46,746 epoch 13: avg loss=4.608537, avg quantization error=0.018884.
2022-03-08 03:03:46,746 begin to evaluate model.
2022-03-08 03:09:45,568 compute mAP.
2022-03-08 03:10:31,197 val mAP=0.829173.
2022-03-08 03:10:31,198 save the best model, db_codes and db_targets.
2022-03-08 03:10:34,229 finish saving.
2022-03-08 03:28:04,599 epoch 14: avg loss=4.604934, avg quantization error=0.018933.
2022-03-08 03:28:04,599 begin to evaluate model.
2022-03-08 03:39:48,190 compute mAP.
2022-03-08 03:40:39,187 val mAP=0.827406.
2022-03-08 03:40:39,188 the monitor loses its patience to 9!.
2022-03-08 04:03:36,409 epoch 15: avg loss=4.598181, avg quantization error=0.019024.
2022-03-08 04:03:36,410 begin to evaluate model.
2022-03-08 04:29:37,040 compute mAP.
2022-03-08 04:30:16,071 val mAP=0.829239.
2022-03-08 04:30:16,072 save the best model, db_codes and db_targets.
2022-03-08 04:30:18,899 finish saving.
2022-03-08 04:46:16,486 epoch 16: avg loss=4.595860, avg quantization error=0.019040.
2022-03-08 04:46:16,487 begin to evaluate model.
2022-03-08 04:54:09,955 compute mAP.
2022-03-08 04:54:51,062 val mAP=0.828661.
2022-03-08 04:54:51,063 the monitor loses its patience to 9!.
2022-03-08 05:12:58,903 epoch 17: avg loss=4.592228, avg quantization error=0.019056.
2022-03-08 05:12:58,903 begin to evaluate model.
2022-03-08 05:23:36,743 compute mAP.
2022-03-08 05:24:19,565 val mAP=0.830686.
2022-03-08 05:24:19,565 save the best model, db_codes and db_targets.
2022-03-08 05:24:22,296 finish saving.
2022-03-08 05:42:01,067 epoch 18: avg loss=4.589605, avg quantization error=0.019133.
2022-03-08 05:42:01,068 begin to evaluate model.
2022-03-08 05:56:20,307 compute mAP.
2022-03-08 05:57:02,740 val mAP=0.829232.
2022-03-08 05:57:02,741 the monitor loses its patience to 9!.
2022-03-08 06:22:52,139 epoch 19: avg loss=4.584395, avg quantization error=0.019192.
2022-03-08 06:22:52,139 begin to evaluate model.
2022-03-08 07:05:58,224 compute mAP.
2022-03-08 07:06:45,750 val mAP=0.829774.
2022-03-08 07:06:45,750 the monitor loses its patience to 8!.
2022-03-08 08:03:57,656 epoch 20: avg loss=4.580817, avg quantization error=0.019207.
2022-03-08 08:03:57,657 begin to evaluate model.
2022-03-08 08:52:53,579 compute mAP.
2022-03-08 08:53:47,603 val mAP=0.830329.
2022-03-08 08:53:47,603 the monitor loses its patience to 7!.
2022-03-08 09:42:44,537 epoch 21: avg loss=4.575198, avg quantization error=0.019252.
2022-03-08 09:42:44,537 begin to evaluate model.
2022-03-08 10:25:15,666 compute mAP.
2022-03-08 10:25:55,403 val mAP=0.829360.
2022-03-08 10:25:55,404 the monitor loses its patience to 6!.
2022-03-08 11:12:54,247 epoch 22: avg loss=4.571899, avg quantization error=0.019325.
2022-03-08 11:12:54,247 begin to evaluate model.
2022-03-08 11:17:45,496 compute mAP.
2022-03-08 11:17:51,879 val mAP=0.829176.
2022-03-08 11:17:51,880 the monitor loses its patience to 5!.
2022-03-08 11:29:31,312 epoch 23: avg loss=4.567406, avg quantization error=0.019394.
2022-03-08 11:29:31,312 begin to evaluate model.
2022-03-08 11:34:23,192 compute mAP.
2022-03-08 11:34:29,413 val mAP=0.829241.
2022-03-08 11:34:29,413 the monitor loses its patience to 4!.
2022-03-08 11:45:29,056 epoch 24: avg loss=4.564545, avg quantization error=0.019412.
2022-03-08 11:45:29,056 begin to evaluate model.
2022-03-08 11:50:19,477 compute mAP.
2022-03-08 11:50:25,481 val mAP=0.829612.
2022-03-08 11:50:25,481 the monitor loses its patience to 3!.
2022-03-08 12:01:28,272 epoch 25: avg loss=4.559009, avg quantization error=0.019468.
2022-03-08 12:01:28,272 begin to evaluate model.
2022-03-08 12:06:19,010 compute mAP.
2022-03-08 12:06:25,577 val mAP=0.827546.
2022-03-08 12:06:25,578 the monitor loses its patience to 2!.
2022-03-08 12:17:28,385 epoch 26: avg loss=4.557904, avg quantization error=0.019502.
2022-03-08 12:17:28,386 begin to evaluate model.
2022-03-08 12:22:19,534 compute mAP.
2022-03-08 12:22:25,724 val mAP=0.829151.
2022-03-08 12:22:25,724 the monitor loses its patience to 1!.
2022-03-08 12:33:24,241 epoch 27: avg loss=4.552271, avg quantization error=0.019543.
2022-03-08 12:33:24,241 begin to evaluate model.
2022-03-08 12:38:16,598 compute mAP.
2022-03-08 12:38:22,846 val mAP=0.830548.
2022-03-08 12:38:22,847 the monitor loses its patience to 0!.
2022-03-08 12:38:22,847 early stop.
2022-03-08 12:38:22,847 free the queue memory.
2022-03-08 12:38:22,847 finish trainning at epoch 27.
2022-03-08 12:38:22,863 finish training, now load the best model and codes.
2022-03-08 12:38:24,588 begin to test model.
2022-03-08 12:38:24,588 compute mAP.
2022-03-08 12:38:31,141 test mAP=0.830686.
2022-03-08 12:38:31,141 compute PR curve and P@top5000 curve.
2022-03-08 12:38:44,779 finish testing.
2022-03-08 12:38:44,779 finish all procedures.
