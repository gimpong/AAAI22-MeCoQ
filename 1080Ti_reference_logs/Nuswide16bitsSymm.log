2022-03-08 13:30:29,189 config: Namespace(K=256, M=2, T=0.2, alpha=10, batch_size=128, checkpoint_root='./checkpoints/Nuswide16bitsSymm', dataset='NUSWIDE', device='cuda:1', download_cifar10=False, epoch_num=50, eval_interval=1, feat_dim=64, final_lr=1e-05, hp_beta=0.01, hp_gamma=0.5, hp_lambda=1.0, is_asym_dist=False, lr=0.01, lr_scaling=0.001, mode='debias', momentum=0.9, monitor_counter=10, notes='Nuswide16bitsSymm', num_workers=10, optimizer='SGD', pos_prior=0.15, protocal='I', queue_begin_epoch=10, seed=2021, start_lr=1e-05, topK=5000, trainable_layer_num=0, use_scheduler=True, use_writer=True, vgg_model_path=None, warmup_epoch_num=1).
2022-03-08 13:30:29,189 prepare NUSWIDE datatset.
2022-03-08 13:30:41,441 setup model.
2022-03-08 13:30:46,342 define loss function.
2022-03-08 13:30:46,343 setup SGD optimizer.
2022-03-08 13:30:46,356 prepare monitor and evaluator.
2022-03-08 13:30:46,358 begin to train model.
2022-03-08 13:30:46,359 register queue.
2022-03-08 14:06:17,456 epoch 0: avg loss=2.956960, avg quantization error=0.008707.
2022-03-08 14:06:17,457 begin to evaluate model.
2022-03-08 14:15:07,866 compute mAP.
2022-03-08 14:15:16,915 val mAP=0.758496.
2022-03-08 14:15:16,916 save the best model, db_codes and db_targets.
2022-03-08 14:15:17,732 finish saving.
2022-03-08 14:55:00,865 epoch 1: avg loss=2.365996, avg quantization error=0.006764.
2022-03-08 14:55:00,865 begin to evaluate model.
2022-03-08 15:03:52,384 compute mAP.
2022-03-08 15:04:01,239 val mAP=0.758202.
2022-03-08 15:04:01,239 the monitor loses its patience to 9!.
2022-03-08 15:46:50,349 epoch 2: avg loss=2.350415, avg quantization error=0.006652.
2022-03-08 15:46:50,350 begin to evaluate model.
2022-03-08 15:55:46,776 compute mAP.
2022-03-08 15:55:56,000 val mAP=0.756324.
2022-03-08 15:55:56,001 the monitor loses its patience to 8!.
2022-03-08 16:36:43,980 epoch 3: avg loss=2.330305, avg quantization error=0.006559.
2022-03-08 16:36:43,980 begin to evaluate model.
2022-03-08 16:45:41,587 compute mAP.
2022-03-08 16:45:50,815 val mAP=0.754912.
2022-03-08 16:45:50,816 the monitor loses its patience to 7!.
2022-03-08 17:32:51,094 epoch 4: avg loss=2.314563, avg quantization error=0.006453.
2022-03-08 17:32:51,094 begin to evaluate model.
2022-03-08 17:41:55,577 compute mAP.
2022-03-08 17:42:05,232 val mAP=0.751250.
2022-03-08 17:42:05,233 the monitor loses its patience to 6!.
2022-03-08 18:35:25,384 epoch 5: avg loss=2.305998, avg quantization error=0.006368.
2022-03-08 18:35:25,384 begin to evaluate model.
2022-03-08 18:44:32,850 compute mAP.
2022-03-08 18:44:42,499 val mAP=0.747746.
2022-03-08 18:44:42,500 the monitor loses its patience to 5!.
2022-03-08 19:31:19,191 epoch 6: avg loss=2.294305, avg quantization error=0.006325.
2022-03-08 19:31:19,191 begin to evaluate model.
2022-03-08 19:40:30,818 compute mAP.
2022-03-08 19:40:40,436 val mAP=0.748957.
2022-03-08 19:40:40,438 the monitor loses its patience to 4!.
2022-03-08 20:29:51,710 epoch 7: avg loss=2.285185, avg quantization error=0.006293.
2022-03-08 20:29:51,723 begin to evaluate model.
2022-03-08 20:38:59,367 compute mAP.
2022-03-08 20:39:09,300 val mAP=0.751824.
2022-03-08 20:39:09,301 the monitor loses its patience to 3!.
2022-03-08 21:32:46,496 epoch 8: avg loss=2.292008, avg quantization error=0.006277.
2022-03-08 21:32:46,497 begin to evaluate model.
2022-03-08 21:41:56,419 compute mAP.
2022-03-08 21:42:06,099 val mAP=0.752503.
2022-03-08 21:42:06,100 the monitor loses its patience to 2!.
2022-03-08 22:38:00,643 epoch 9: avg loss=2.285580, avg quantization error=0.006257.
2022-03-08 22:38:00,644 begin to evaluate model.
2022-03-08 22:47:09,746 compute mAP.
2022-03-08 22:47:19,676 val mAP=0.751470.
2022-03-08 22:47:19,677 the monitor loses its patience to 1!.
2022-03-08 23:33:14,066 epoch 10: avg loss=5.335416, avg quantization error=0.006473.
2022-03-08 23:33:14,066 begin to evaluate model.
2022-03-08 23:42:21,823 compute mAP.
2022-03-08 23:42:31,311 val mAP=0.750822.
2022-03-08 23:42:31,312 the monitor loses its patience to 0!.
2022-03-08 23:42:31,313 early stop.
2022-03-08 23:42:31,313 free the queue memory.
2022-03-08 23:42:31,313 finish trainning at epoch 10.
2022-03-08 23:42:31,333 finish training, now load the best model and codes.
2022-03-08 23:42:34,860 begin to test model.
2022-03-08 23:42:34,860 compute mAP.
2022-03-08 23:42:44,550 test mAP=0.758496.
2022-03-08 23:42:44,550 compute PR curve and P@top5000 curve.
