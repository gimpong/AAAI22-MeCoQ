2022-03-08 19:51:03,777 config: Namespace(K=256, M=4, T=0.35, alpha=10, batch_size=128, checkpoint_root='./checkpoints/CifarII32bits', dataset='CIFAR10', device='cuda:0', download_cifar10=False, epoch_num=50, eval_interval=1, feat_dim=48, final_lr=1e-05, hp_beta=0.005, hp_gamma=0.5, hp_lambda=0.1, is_asym_dist=True, lr=0.01, lr_scaling=0.001, mode='debias', momentum=0.9, monitor_counter=10, notes='CifarII32bits', num_workers=10, optimizer='SGD', pos_prior=0.1, protocal='II', queue_begin_epoch=15, seed=2021, start_lr=1e-05, topK=1000, trainable_layer_num=2, use_scheduler=True, use_writer=True, vgg_model_path=None, warmup_epoch_num=1).
2022-03-08 19:51:03,778 prepare CIFAR10 datatset.
2022-03-08 19:51:04,871 setup model.
2022-03-08 19:51:08,500 define loss function.
2022-03-08 19:51:08,501 setup SGD optimizer.
2022-03-08 19:51:08,501 prepare monitor and evaluator.
2022-03-08 19:51:08,501 begin to train model.
2022-03-08 19:51:08,502 register queue.
2022-03-08 19:52:01,694 epoch 0: avg loss=4.469045, avg quantization error=0.018729.
2022-03-08 19:52:01,694 begin to evaluate model.
2022-03-08 19:54:21,178 compute mAP.
2022-03-08 19:54:51,996 val mAP=0.534606.
2022-03-08 19:54:51,997 save the best model, db_codes and db_targets.
2022-03-08 19:54:52,659 finish saving.
2022-03-08 19:56:00,823 epoch 1: avg loss=3.306184, avg quantization error=0.016147.
2022-03-08 19:56:00,823 begin to evaluate model.
2022-03-08 19:58:20,196 compute mAP.
2022-03-08 19:58:50,887 val mAP=0.561451.
2022-03-08 19:58:50,887 save the best model, db_codes and db_targets.
2022-03-08 19:58:52,018 finish saving.
2022-03-08 19:59:57,745 epoch 2: avg loss=3.046043, avg quantization error=0.015635.
2022-03-08 19:59:57,746 begin to evaluate model.
2022-03-08 20:02:16,925 compute mAP.
2022-03-08 20:02:47,424 val mAP=0.571852.
2022-03-08 20:02:47,425 save the best model, db_codes and db_targets.
2022-03-08 20:02:48,588 finish saving.
2022-03-08 20:03:55,101 epoch 3: avg loss=2.897817, avg quantization error=0.015479.
2022-03-08 20:03:55,101 begin to evaluate model.
2022-03-08 20:06:14,843 compute mAP.
2022-03-08 20:06:45,340 val mAP=0.579709.
2022-03-08 20:06:45,341 save the best model, db_codes and db_targets.
2022-03-08 20:06:46,484 finish saving.
2022-03-08 20:07:54,467 epoch 4: avg loss=2.788716, avg quantization error=0.015240.
2022-03-08 20:07:54,467 begin to evaluate model.
2022-03-08 20:10:14,259 compute mAP.
2022-03-08 20:10:45,097 val mAP=0.593466.
2022-03-08 20:10:45,098 save the best model, db_codes and db_targets.
2022-03-08 20:10:46,216 finish saving.
2022-03-08 20:11:53,544 epoch 5: avg loss=2.695300, avg quantization error=0.015346.
2022-03-08 20:11:53,544 begin to evaluate model.
2022-03-08 20:14:13,706 compute mAP.
2022-03-08 20:14:44,570 val mAP=0.584844.
2022-03-08 20:14:44,571 the monitor loses its patience to 9!.
2022-03-08 20:15:50,250 epoch 6: avg loss=2.612362, avg quantization error=0.015351.
2022-03-08 20:15:50,251 begin to evaluate model.
2022-03-08 20:18:09,894 compute mAP.
2022-03-08 20:18:40,294 val mAP=0.599761.
2022-03-08 20:18:40,295 save the best model, db_codes and db_targets.
2022-03-08 20:18:41,405 finish saving.
2022-03-08 20:19:48,692 epoch 7: avg loss=2.555234, avg quantization error=0.015148.
2022-03-08 20:19:48,693 begin to evaluate model.
2022-03-08 20:22:08,383 compute mAP.
2022-03-08 20:22:38,977 val mAP=0.597319.
2022-03-08 20:22:38,978 the monitor loses its patience to 9!.
2022-03-08 20:23:42,705 epoch 8: avg loss=2.457611, avg quantization error=0.015057.
2022-03-08 20:23:42,706 begin to evaluate model.
2022-03-08 20:26:02,777 compute mAP.
2022-03-08 20:26:33,294 val mAP=0.605718.
2022-03-08 20:26:33,295 save the best model, db_codes and db_targets.
2022-03-08 20:26:34,449 finish saving.
2022-03-08 20:27:44,600 epoch 9: avg loss=2.434613, avg quantization error=0.015109.
2022-03-08 20:27:44,600 begin to evaluate model.
2022-03-08 20:30:04,440 compute mAP.
2022-03-08 20:30:35,330 val mAP=0.609464.
2022-03-08 20:30:35,331 save the best model, db_codes and db_targets.
2022-03-08 20:30:36,424 finish saving.
2022-03-08 20:31:44,150 epoch 10: avg loss=2.358121, avg quantization error=0.014991.
2022-03-08 20:31:44,150 begin to evaluate model.
2022-03-08 20:34:04,039 compute mAP.
2022-03-08 20:34:34,753 val mAP=0.613782.
2022-03-08 20:34:34,754 save the best model, db_codes and db_targets.
2022-03-08 20:34:35,875 finish saving.
2022-03-08 20:35:40,766 epoch 11: avg loss=2.333832, avg quantization error=0.015008.
2022-03-08 20:35:40,767 begin to evaluate model.
2022-03-08 20:38:00,574 compute mAP.
2022-03-08 20:38:31,496 val mAP=0.610655.
2022-03-08 20:38:31,497 the monitor loses its patience to 9!.
2022-03-08 20:39:38,044 epoch 12: avg loss=2.325030, avg quantization error=0.014987.
2022-03-08 20:39:38,044 begin to evaluate model.
2022-03-08 20:41:57,616 compute mAP.
2022-03-08 20:42:28,562 val mAP=0.622215.
2022-03-08 20:42:28,563 save the best model, db_codes and db_targets.
2022-03-08 20:42:29,976 finish saving.
2022-03-08 20:43:36,395 epoch 13: avg loss=2.261134, avg quantization error=0.014980.
2022-03-08 20:43:36,396 begin to evaluate model.
2022-03-08 20:45:56,130 compute mAP.
2022-03-08 20:46:26,878 val mAP=0.623205.
2022-03-08 20:46:26,878 save the best model, db_codes and db_targets.
2022-03-08 20:46:28,002 finish saving.
2022-03-08 20:47:33,611 epoch 14: avg loss=2.220417, avg quantization error=0.014988.
2022-03-08 20:47:33,611 begin to evaluate model.
2022-03-08 20:49:53,424 compute mAP.
2022-03-08 20:50:24,113 val mAP=0.624613.
2022-03-08 20:50:24,114 save the best model, db_codes and db_targets.
2022-03-08 20:50:25,240 finish saving.
2022-03-08 20:51:30,494 epoch 15: avg loss=4.910065, avg quantization error=0.015039.
2022-03-08 20:51:30,494 begin to evaluate model.
2022-03-08 20:53:50,338 compute mAP.
2022-03-08 20:54:21,081 val mAP=0.626636.
2022-03-08 20:54:21,082 save the best model, db_codes and db_targets.
2022-03-08 20:54:22,301 finish saving.
2022-03-08 20:55:26,518 epoch 16: avg loss=4.885570, avg quantization error=0.015141.
2022-03-08 20:55:26,518 begin to evaluate model.
2022-03-08 20:57:46,364 compute mAP.
2022-03-08 20:58:17,062 val mAP=0.628923.
2022-03-08 20:58:17,063 save the best model, db_codes and db_targets.
2022-03-08 20:58:18,208 finish saving.
2022-03-08 20:59:24,203 epoch 17: avg loss=4.850652, avg quantization error=0.015040.
2022-03-08 20:59:24,203 begin to evaluate model.
2022-03-08 21:01:43,995 compute mAP.
2022-03-08 21:02:14,729 val mAP=0.629960.
2022-03-08 21:02:14,730 save the best model, db_codes and db_targets.
2022-03-08 21:02:15,848 finish saving.
2022-03-08 21:03:21,254 epoch 18: avg loss=4.846835, avg quantization error=0.015016.
2022-03-08 21:03:21,255 begin to evaluate model.
2022-03-08 21:05:40,888 compute mAP.
2022-03-08 21:06:11,453 val mAP=0.629922.
2022-03-08 21:06:11,454 the monitor loses its patience to 9!.
2022-03-08 21:07:15,290 epoch 19: avg loss=4.832259, avg quantization error=0.014947.
2022-03-08 21:07:15,290 begin to evaluate model.
2022-03-08 21:09:35,376 compute mAP.
2022-03-08 21:10:06,460 val mAP=0.630764.
2022-03-08 21:10:06,461 save the best model, db_codes and db_targets.
2022-03-08 21:10:07,801 finish saving.
2022-03-08 21:11:17,374 epoch 20: avg loss=4.829898, avg quantization error=0.014792.
2022-03-08 21:11:17,374 begin to evaluate model.
2022-03-08 21:13:38,030 compute mAP.
2022-03-08 21:14:08,700 val mAP=0.633959.
2022-03-08 21:14:08,700 save the best model, db_codes and db_targets.
2022-03-08 21:14:09,796 finish saving.
2022-03-08 21:15:15,503 epoch 21: avg loss=4.820685, avg quantization error=0.014816.
2022-03-08 21:15:15,504 begin to evaluate model.
2022-03-08 21:17:35,990 compute mAP.
2022-03-08 21:18:06,644 val mAP=0.631075.
2022-03-08 21:18:06,645 the monitor loses its patience to 9!.
2022-03-08 21:19:12,060 epoch 22: avg loss=4.803532, avg quantization error=0.014848.
2022-03-08 21:19:12,060 begin to evaluate model.
2022-03-08 21:21:31,879 compute mAP.
2022-03-08 21:22:02,730 val mAP=0.632261.
2022-03-08 21:22:02,731 the monitor loses its patience to 8!.
2022-03-08 21:23:09,923 epoch 23: avg loss=4.776466, avg quantization error=0.014822.
2022-03-08 21:23:09,924 begin to evaluate model.
2022-03-08 21:25:29,984 compute mAP.
2022-03-08 21:26:00,561 val mAP=0.632536.
2022-03-08 21:26:00,562 the monitor loses its patience to 7!.
2022-03-08 21:27:08,060 epoch 24: avg loss=4.798076, avg quantization error=0.014749.
2022-03-08 21:27:08,061 begin to evaluate model.
2022-03-08 21:29:27,878 compute mAP.
2022-03-08 21:29:58,900 val mAP=0.636726.
2022-03-08 21:29:58,901 save the best model, db_codes and db_targets.
2022-03-08 21:30:00,263 finish saving.
2022-03-08 21:31:00,905 epoch 25: avg loss=4.797506, avg quantization error=0.014576.
2022-03-08 21:31:00,905 begin to evaluate model.
2022-03-08 21:33:20,444 compute mAP.
2022-03-08 21:33:51,094 val mAP=0.634085.
2022-03-08 21:33:51,095 the monitor loses its patience to 9!.
2022-03-08 21:34:58,711 epoch 26: avg loss=4.793055, avg quantization error=0.014629.
2022-03-08 21:34:58,712 begin to evaluate model.
2022-03-08 21:37:17,895 compute mAP.
2022-03-08 21:37:48,915 val mAP=0.636983.
2022-03-08 21:37:48,916 save the best model, db_codes and db_targets.
2022-03-08 21:37:50,071 finish saving.
2022-03-08 21:38:52,064 epoch 27: avg loss=4.784384, avg quantization error=0.014649.
2022-03-08 21:38:52,064 begin to evaluate model.
2022-03-08 21:41:12,157 compute mAP.
2022-03-08 21:41:43,079 val mAP=0.636343.
2022-03-08 21:41:43,079 the monitor loses its patience to 9!.
2022-03-08 21:42:48,036 epoch 28: avg loss=4.777392, avg quantization error=0.014602.
2022-03-08 21:42:48,037 begin to evaluate model.
2022-03-08 21:45:07,925 compute mAP.
2022-03-08 21:45:38,440 val mAP=0.639251.
2022-03-08 21:45:38,441 save the best model, db_codes and db_targets.
2022-03-08 21:45:39,796 finish saving.
2022-03-08 21:46:46,401 epoch 29: avg loss=4.777997, avg quantization error=0.014555.
2022-03-08 21:46:46,401 begin to evaluate model.
2022-03-08 21:49:06,360 compute mAP.
2022-03-08 21:49:37,447 val mAP=0.637661.
2022-03-08 21:49:37,447 the monitor loses its patience to 9!.
2022-03-08 21:50:41,466 epoch 30: avg loss=4.761879, avg quantization error=0.014478.
2022-03-08 21:50:41,467 begin to evaluate model.
2022-03-08 21:53:01,465 compute mAP.
2022-03-08 21:53:32,425 val mAP=0.639072.
2022-03-08 21:53:32,426 the monitor loses its patience to 8!.
2022-03-08 21:54:35,243 epoch 31: avg loss=4.763421, avg quantization error=0.014487.
2022-03-08 21:54:35,243 begin to evaluate model.
2022-03-08 21:56:54,861 compute mAP.
2022-03-08 21:57:25,596 val mAP=0.638536.
2022-03-08 21:57:25,596 the monitor loses its patience to 7!.
2022-03-08 21:58:33,518 epoch 32: avg loss=4.758072, avg quantization error=0.014457.
2022-03-08 21:58:33,519 begin to evaluate model.
2022-03-08 22:00:53,218 compute mAP.
2022-03-08 22:01:23,586 val mAP=0.639760.
2022-03-08 22:01:23,586 save the best model, db_codes and db_targets.
2022-03-08 22:01:25,058 finish saving.
2022-03-08 22:02:30,496 epoch 33: avg loss=4.746775, avg quantization error=0.014479.
2022-03-08 22:02:30,497 begin to evaluate model.
2022-03-08 22:04:51,023 compute mAP.
2022-03-08 22:05:21,739 val mAP=0.640448.
2022-03-08 22:05:21,740 save the best model, db_codes and db_targets.
2022-03-08 22:05:23,164 finish saving.
2022-03-08 22:06:26,306 epoch 34: avg loss=4.742217, avg quantization error=0.014467.
2022-03-08 22:06:26,307 begin to evaluate model.
2022-03-08 22:08:45,951 compute mAP.
2022-03-08 22:09:16,748 val mAP=0.640442.
2022-03-08 22:09:16,749 the monitor loses its patience to 9!.
2022-03-08 22:10:19,202 epoch 35: avg loss=4.748776, avg quantization error=0.014501.
2022-03-08 22:10:19,203 begin to evaluate model.
2022-03-08 22:12:38,988 compute mAP.
2022-03-08 22:13:09,676 val mAP=0.639818.
2022-03-08 22:13:09,677 the monitor loses its patience to 8!.
2022-03-08 22:14:14,354 epoch 36: avg loss=4.738821, avg quantization error=0.014460.
2022-03-08 22:14:14,354 begin to evaluate model.
2022-03-08 22:16:34,191 compute mAP.
2022-03-08 22:17:05,066 val mAP=0.640456.
2022-03-08 22:17:05,067 save the best model, db_codes and db_targets.
2022-03-08 22:17:06,775 finish saving.
2022-03-08 22:18:10,905 epoch 37: avg loss=4.742624, avg quantization error=0.014471.
2022-03-08 22:18:10,906 begin to evaluate model.
2022-03-08 22:20:30,445 compute mAP.
2022-03-08 22:21:01,063 val mAP=0.639893.
2022-03-08 22:21:01,064 the monitor loses its patience to 9!.
2022-03-08 22:22:05,363 epoch 38: avg loss=4.728082, avg quantization error=0.014449.
2022-03-08 22:22:05,363 begin to evaluate model.
2022-03-08 22:24:24,980 compute mAP.
2022-03-08 22:24:55,652 val mAP=0.639747.
2022-03-08 22:24:55,653 the monitor loses its patience to 8!.
2022-03-08 22:26:05,907 epoch 39: avg loss=4.726491, avg quantization error=0.014446.
2022-03-08 22:26:05,907 begin to evaluate model.
2022-03-08 22:28:25,707 compute mAP.
2022-03-08 22:28:56,399 val mAP=0.639760.
2022-03-08 22:28:56,400 the monitor loses its patience to 7!.
2022-03-08 22:30:04,158 epoch 40: avg loss=4.728777, avg quantization error=0.014384.
2022-03-08 22:30:04,158 begin to evaluate model.
2022-03-08 22:32:23,842 compute mAP.
2022-03-08 22:32:54,893 val mAP=0.640333.
2022-03-08 22:32:54,893 the monitor loses its patience to 6!.
2022-03-08 22:34:04,412 epoch 41: avg loss=4.720949, avg quantization error=0.014400.
2022-03-08 22:34:04,413 begin to evaluate model.
2022-03-08 22:36:24,214 compute mAP.
2022-03-08 22:36:54,809 val mAP=0.640419.
2022-03-08 22:36:54,809 the monitor loses its patience to 5!.
2022-03-08 22:38:04,759 epoch 42: avg loss=4.742600, avg quantization error=0.014382.
2022-03-08 22:38:04,759 begin to evaluate model.
2022-03-08 22:40:24,547 compute mAP.
2022-03-08 22:40:55,079 val mAP=0.640929.
2022-03-08 22:40:55,080 save the best model, db_codes and db_targets.
2022-03-08 22:40:56,535 finish saving.
2022-03-08 22:42:02,994 epoch 43: avg loss=4.716710, avg quantization error=0.014399.
2022-03-08 22:42:02,994 begin to evaluate model.
2022-03-08 22:44:22,960 compute mAP.
2022-03-08 22:44:54,071 val mAP=0.640486.
2022-03-08 22:44:54,072 the monitor loses its patience to 9!.
2022-03-08 22:46:02,478 epoch 44: avg loss=4.733469, avg quantization error=0.014428.
2022-03-08 22:46:02,478 begin to evaluate model.
2022-03-08 22:48:21,987 compute mAP.
2022-03-08 22:48:52,916 val mAP=0.640432.
2022-03-08 22:48:52,917 the monitor loses its patience to 8!.
2022-03-08 22:49:57,397 epoch 45: avg loss=4.726575, avg quantization error=0.014384.
2022-03-08 22:49:57,397 begin to evaluate model.
2022-03-08 22:52:16,640 compute mAP.
2022-03-08 22:52:47,263 val mAP=0.640205.
2022-03-08 22:52:47,264 the monitor loses its patience to 7!.
2022-03-08 22:53:52,747 epoch 46: avg loss=4.727391, avg quantization error=0.014394.
2022-03-08 22:53:52,747 begin to evaluate model.
2022-03-08 22:56:12,516 compute mAP.
2022-03-08 22:56:43,189 val mAP=0.640307.
2022-03-08 22:56:43,190 the monitor loses its patience to 6!.
2022-03-08 22:57:50,195 epoch 47: avg loss=4.712640, avg quantization error=0.014397.
2022-03-08 22:57:50,195 begin to evaluate model.
2022-03-08 23:00:10,002 compute mAP.
2022-03-08 23:00:40,682 val mAP=0.640428.
2022-03-08 23:00:40,682 the monitor loses its patience to 5!.
2022-03-08 23:01:48,179 epoch 48: avg loss=4.730047, avg quantization error=0.014410.
2022-03-08 23:01:48,180 begin to evaluate model.
2022-03-08 23:04:08,056 compute mAP.
2022-03-08 23:04:39,036 val mAP=0.640388.
2022-03-08 23:04:39,037 the monitor loses its patience to 4!.
2022-03-08 23:05:46,040 epoch 49: avg loss=4.723113, avg quantization error=0.014386.
2022-03-08 23:05:46,040 begin to evaluate model.
2022-03-08 23:08:06,251 compute mAP.
2022-03-08 23:08:36,749 val mAP=0.640410.
2022-03-08 23:08:36,749 the monitor loses its patience to 3!.
2022-03-08 23:08:36,750 free the queue memory.
2022-03-08 23:08:36,750 finish trainning at epoch 49.
2022-03-08 23:08:36,752 finish training, now load the best model and codes.
2022-03-08 23:08:37,268 begin to test model.
2022-03-08 23:08:37,268 compute mAP.
2022-03-08 23:09:07,815 test mAP=0.640929.
2022-03-08 23:09:07,815 compute PR curve and P@top1000 curve.
2022-03-08 23:10:10,044 finish testing.
2022-03-08 23:10:10,044 finish all procedures.
