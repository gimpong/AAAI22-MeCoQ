2022-03-09 21:29:27,794 config: Namespace(K=256, M=2, T=0.4, alpha=10, batch_size=128, checkpoint_root='./checkpoints/Flickr16bits', dataset='Flickr25K', device='cuda:1', download_cifar10=False, epoch_num=50, eval_interval=1, feat_dim=32, final_lr=1e-05, hp_beta=0.1, hp_gamma=0.5, hp_lambda=0.5, is_asym_dist=True, lr=0.01, lr_scaling=0.001, mode='debias', momentum=0.9, monitor_counter=10, notes='Flickr16bits', num_workers=10, optimizer='SGD', pos_prior=0.15, protocal='I', queue_begin_epoch=5, seed=2021, start_lr=1e-05, topK=5000, trainable_layer_num=0, use_scheduler=True, use_writer=True, vgg_model_path=None, warmup_epoch_num=1).
2022-03-09 21:29:27,795 prepare Flickr25K datatset.
2022-03-09 21:29:28,180 setup model.
2022-03-09 21:29:35,701 define loss function.
2022-03-09 21:29:35,701 setup SGD optimizer.
2022-03-09 21:29:35,702 prepare monitor and evaluator.
2022-03-09 21:29:35,702 begin to train model.
2022-03-09 21:29:35,703 register queue.
2022-03-09 21:30:50,617 epoch 0: avg loss=5.009851, avg quantization error=0.017778.
2022-03-09 21:30:50,617 begin to evaluate model.
2022-03-09 21:32:37,763 compute mAP.
2022-03-09 21:32:54,707 val mAP=0.783052.
2022-03-09 21:32:54,707 save the best model, db_codes and db_targets.
2022-03-09 21:32:55,474 finish saving.
2022-03-09 21:34:43,497 epoch 1: avg loss=3.289378, avg quantization error=0.015930.
2022-03-09 21:34:43,497 begin to evaluate model.
2022-03-09 21:35:30,761 compute mAP.
2022-03-09 21:35:36,535 val mAP=0.780988.
2022-03-09 21:35:36,536 the monitor loses its patience to 9!.
2022-03-09 21:37:17,449 epoch 2: avg loss=3.069151, avg quantization error=0.015714.
2022-03-09 21:37:17,450 begin to evaluate model.
2022-03-09 21:38:04,435 compute mAP.
2022-03-09 21:38:10,189 val mAP=0.788724.
2022-03-09 21:38:10,190 save the best model, db_codes and db_targets.
2022-03-09 21:38:14,432 finish saving.
2022-03-09 21:39:58,030 epoch 3: avg loss=2.993108, avg quantization error=0.015356.
2022-03-09 21:39:58,031 begin to evaluate model.
2022-03-09 21:40:45,522 compute mAP.
2022-03-09 21:40:51,446 val mAP=0.793948.
2022-03-09 21:40:51,447 save the best model, db_codes and db_targets.
2022-03-09 21:40:55,332 finish saving.
2022-03-09 21:42:48,712 epoch 4: avg loss=2.958863, avg quantization error=0.015300.
2022-03-09 21:42:48,713 begin to evaluate model.
2022-03-09 21:43:36,503 compute mAP.
2022-03-09 21:43:42,435 val mAP=0.795161.
2022-03-09 21:43:42,436 save the best model, db_codes and db_targets.
2022-03-09 21:43:46,310 finish saving.
2022-03-09 21:45:39,138 epoch 5: avg loss=5.789395, avg quantization error=0.014781.
2022-03-09 21:45:39,138 begin to evaluate model.
2022-03-09 21:46:27,021 compute mAP.
2022-03-09 21:46:33,181 val mAP=0.796723.
2022-03-09 21:46:33,182 save the best model, db_codes and db_targets.
2022-03-09 21:46:36,242 finish saving.
2022-03-09 21:48:32,302 epoch 6: avg loss=5.761457, avg quantization error=0.013922.
2022-03-09 21:48:32,303 begin to evaluate model.
2022-03-09 21:49:20,157 compute mAP.
2022-03-09 21:49:26,573 val mAP=0.799159.
2022-03-09 21:49:26,574 save the best model, db_codes and db_targets.
2022-03-09 21:49:30,889 finish saving.
2022-03-09 21:51:13,345 epoch 7: avg loss=5.765294, avg quantization error=0.013688.
2022-03-09 21:51:13,345 begin to evaluate model.
2022-03-09 21:52:00,804 compute mAP.
2022-03-09 21:52:07,255 val mAP=0.804552.
2022-03-09 21:52:07,256 save the best model, db_codes and db_targets.
2022-03-09 21:52:11,534 finish saving.
2022-03-09 21:53:44,947 epoch 8: avg loss=5.802131, avg quantization error=0.013614.
2022-03-09 21:53:44,947 begin to evaluate model.
2022-03-09 21:54:32,233 compute mAP.
2022-03-09 21:54:38,799 val mAP=0.799375.
2022-03-09 21:54:38,800 the monitor loses its patience to 9!.
2022-03-09 21:56:25,318 epoch 9: avg loss=5.791025, avg quantization error=0.013555.
2022-03-09 21:56:25,318 begin to evaluate model.
2022-03-09 21:57:13,202 compute mAP.
2022-03-09 21:57:19,206 val mAP=0.807199.
2022-03-09 21:57:19,207 save the best model, db_codes and db_targets.
2022-03-09 21:57:23,909 finish saving.
2022-03-09 21:59:04,481 epoch 10: avg loss=5.773708, avg quantization error=0.013193.
2022-03-09 21:59:04,481 begin to evaluate model.
2022-03-09 21:59:51,652 compute mAP.
2022-03-09 21:59:58,136 val mAP=0.812682.
2022-03-09 21:59:58,137 save the best model, db_codes and db_targets.
2022-03-09 22:00:01,556 finish saving.
2022-03-09 22:01:32,192 epoch 11: avg loss=5.776214, avg quantization error=0.013107.
2022-03-09 22:01:32,192 begin to evaluate model.
2022-03-09 22:02:19,725 compute mAP.
2022-03-09 22:02:26,409 val mAP=0.805520.
2022-03-09 22:02:26,410 the monitor loses its patience to 9!.
2022-03-09 22:04:11,797 epoch 12: avg loss=5.744310, avg quantization error=0.012925.
2022-03-09 22:04:11,797 begin to evaluate model.
2022-03-09 22:04:59,674 compute mAP.
2022-03-09 22:05:06,421 val mAP=0.811236.
2022-03-09 22:05:06,421 the monitor loses its patience to 8!.
2022-03-09 22:06:53,324 epoch 13: avg loss=5.758206, avg quantization error=0.013109.
2022-03-09 22:06:53,325 begin to evaluate model.
2022-03-09 22:07:40,815 compute mAP.
2022-03-09 22:07:47,115 val mAP=0.800847.
2022-03-09 22:07:47,116 the monitor loses its patience to 7!.
2022-03-09 22:09:25,541 epoch 14: avg loss=5.771695, avg quantization error=0.013012.
2022-03-09 22:09:25,541 begin to evaluate model.
2022-03-09 22:10:13,108 compute mAP.
2022-03-09 22:10:19,806 val mAP=0.803298.
2022-03-09 22:10:19,807 the monitor loses its patience to 6!.
2022-03-09 22:12:06,077 epoch 15: avg loss=5.758513, avg quantization error=0.012959.
2022-03-09 22:12:06,078 begin to evaluate model.
2022-03-09 22:12:53,661 compute mAP.
2022-03-09 22:13:00,073 val mAP=0.802177.
2022-03-09 22:13:00,074 the monitor loses its patience to 5!.
2022-03-09 22:14:46,798 epoch 16: avg loss=5.769623, avg quantization error=0.012903.
2022-03-09 22:14:46,798 begin to evaluate model.
2022-03-09 22:15:34,575 compute mAP.
2022-03-09 22:15:41,235 val mAP=0.803572.
2022-03-09 22:15:41,235 the monitor loses its patience to 4!.
2022-03-09 22:17:48,913 epoch 17: avg loss=5.752932, avg quantization error=0.012695.
2022-03-09 22:17:48,913 begin to evaluate model.
2022-03-09 22:18:36,877 compute mAP.
2022-03-09 22:18:43,642 val mAP=0.810516.
2022-03-09 22:18:43,643 the monitor loses its patience to 3!.
2022-03-09 22:20:20,971 epoch 18: avg loss=5.756783, avg quantization error=0.012771.
2022-03-09 22:20:20,971 begin to evaluate model.
2022-03-09 22:21:08,795 compute mAP.
2022-03-09 22:21:14,619 val mAP=0.806559.
2022-03-09 22:21:14,619 the monitor loses its patience to 2!.
2022-03-09 22:22:59,185 epoch 19: avg loss=5.731925, avg quantization error=0.012605.
2022-03-09 22:22:59,185 begin to evaluate model.
2022-03-09 22:23:46,603 compute mAP.
2022-03-09 22:23:52,536 val mAP=0.811247.
2022-03-09 22:23:52,536 the monitor loses its patience to 1!.
2022-03-09 22:25:32,935 epoch 20: avg loss=5.737075, avg quantization error=0.012499.
2022-03-09 22:25:32,936 begin to evaluate model.
2022-03-09 22:26:21,513 compute mAP.
2022-03-09 22:26:27,372 val mAP=0.806788.
2022-03-09 22:26:27,373 the monitor loses its patience to 0!.
2022-03-09 22:26:27,373 early stop.
2022-03-09 22:26:27,373 free the queue memory.
2022-03-09 22:26:27,373 finish trainning at epoch 20.
2022-03-09 22:26:27,375 finish training, now load the best model and codes.
2022-03-09 22:26:27,920 begin to test model.
2022-03-09 22:26:27,920 compute mAP.
2022-03-09 22:26:33,628 test mAP=0.812682.
2022-03-09 22:26:33,628 compute PR curve and P@top5000 curve.
2022-03-09 22:26:45,586 finish testing.
2022-03-09 22:26:45,587 finish all procedures.
